# Embeddings And Vector Search

Last reviewed: 2026-02-10

[Contents](README.md) | [Prev](11-structured-outputs-and-tool-calling.md) | [Next](17-fine-tuning.md)

## Summary

Embeddings map text (and sometimes images, audio, and code) into dense numerical vectors so that semantic similarity can be computed as geometric distance. Vector search -- finding the nearest neighbors to a query vector in a large collection -- is the backbone of most [retrieval-augmented generation (RAG)](03-rag.md) systems, and is also used for clustering, deduplication, classification, and routing. This chapter explains how embeddings work conceptually, the distance metrics used to compare them, the algorithms that make nearest-neighbor search fast at scale, and the practical design decisions that determine whether your retrieval system works well or fails quietly.

## See Also
- [Retrieval-Augmented Generation (RAG)](03-rag.md)
- [Evals And Testing](05-evals.md)

## When To Use

Embeddings and vector search are the right tool whenever you need to find things by meaning rather than by exact keyword match. The core use cases are:

- **Semantic search over documents.** A user types a question in natural language, and you need to find the most relevant paragraphs from a knowledge base, even when the question and the document use different words for the same concept.
- **Retrieval for RAG.** Before asking an LLM to answer a question, you retrieve relevant context from a vector store so the model can ground its response in real documents rather than its training data.
- **Deduplication and clustering.** Finding near-duplicate documents, grouping similar support tickets, or identifying related items in a catalog.
- **Similarity-based routing.** Selecting which workflow, tool, or specialist model to invoke based on which pre-computed examples are closest to the incoming request.

Vector search is not the right tool for exact match queries (use a database index), highly structured queries (use SQL), or situations where the ranking must be perfectly explainable (embedding similarity is a black box).

## How It Works

### What Embeddings Are

An embedding model takes a piece of content -- a sentence, a paragraph, an image -- and produces a fixed-length vector of floating-point numbers, typically between 256 and 4096 dimensions. This vector is a learned representation that captures the semantic content of the input: pieces of text that mean similar things end up close together in the vector space, while unrelated pieces end up far apart.

The underlying mechanism is a neural network (usually a transformer) that has been trained on large amounts of data to place semantically similar inputs near each other. Training typically uses a contrastive objective: the model sees pairs of related inputs (e.g., a question and its answer, a sentence and its paraphrase) and learns to produce vectors that are close together for related pairs and far apart for unrelated pairs. The [Sentence-BERT](https://arxiv.org/abs/1908.10084) family of models popularized this approach and remains a strong baseline.

A useful mental model: think of the embedding vector as coordinates in a very high-dimensional space. Just as two cities that are geographically close have similar latitude/longitude coordinates, two sentences that are semantically similar have similar embedding vectors. The "dimensions" of this space do not correspond to interpretable features -- they are learned abstractions that the model uses internally to organize meaning.

A runnable example using the [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings) that embeds three sentences and compares their similarity:

```python
from openai import OpenAI

client = OpenAI()

texts = ["How do I reset my password?", "Password recovery help", "What is the weather?"]

response = client.embeddings.create(model="text-embedding-3-small", input=texts)

vectors = [item.embedding for item in response.data]
print(f"Dimensions: {len(vectors[0])}")  # 1536

# Cosine similarity (manual, for illustration)
import numpy as np

def cosine_sim(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

print(f"Similar:   {cosine_sim(vectors[0], vectors[1]):.3f}")  # ~0.85+
print(f"Unrelated: {cosine_sim(vectors[0], vectors[2]):.3f}")  # ~0.5 or lower
```

One important property of embeddings is that they are generated by a specific model, and different models produce different vector spaces. You cannot meaningfully compare vectors from two different embedding models, just as you cannot compare coordinates from two different map projections. This means that if you change your embedding model, you must re-embed your entire corpus and rebuild your index.

### Distance Metrics

Comparing embedding vectors requires a distance or similarity metric. The three most common choices are:

**[Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)** measures the angle between two vectors, ignoring their magnitude. It ranges from -1 (opposite directions) to 1 (same direction), with 0 indicating orthogonality (no similarity). Cosine similarity is the most widely used metric for text embeddings because most embedding models are trained to produce normalized vectors (vectors with unit length), and for normalized vectors, cosine similarity and dot product are equivalent. In practice, you often see cosine distance (1 - cosine similarity) used so that smaller values mean more similar.

**Dot product** (inner product) multiplies corresponding dimensions and sums the results. For normalized vectors it is identical to cosine similarity. For unnormalized vectors, it also accounts for vector magnitude, which can be useful when magnitude carries information (e.g., longer or more detailed documents producing longer vectors). Check whether your embedding model produces normalized output -- if it does, cosine similarity and dot product give the same ranking.

**[Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)** (L2 distance) is the straight-line distance between two points in the vector space. It is intuitive and widely understood, but for high-dimensional normalized vectors, it is monotonically related to cosine distance and offers no additional ranking information. Euclidean distance is more useful when vectors are not normalized or when the absolute position in the vector space matters (e.g., in some clustering applications).

For most text retrieval applications with modern embedding models, cosine similarity is the safe default. Only switch to another metric if your embedding model's documentation recommends it or if you have a specific reason.

### Nearest Neighbor Search

Given a query vector, finding the most similar vectors in a collection is a nearest-neighbor search problem. The brute-force approach -- computing the distance between the query and every vector in the collection -- gives perfect results but scales linearly with collection size. For a million vectors, this is slow. For a billion vectors, it is infeasible.

[Approximate nearest neighbor (ANN)](https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximate_nearest_neighbor) algorithms solve this by building an index structure that allows sub-linear search time, trading a small amount of recall (the fraction of true nearest neighbors that are found) for a large speedup. The two most important ANN algorithms in practice are HNSW and IVF.

**[HNSW (Hierarchical Navigable Small World)](https://arxiv.org/abs/1603.09320)** builds a multi-layered graph over the vectors. The top layer is a sparse graph connecting distant points; each successive layer adds more connections. Searching starts at the top layer and navigates "downward" through increasingly dense layers, following edges that move closer to the query vector at each step. HNSW offers excellent query performance (typically sub-millisecond for millions of vectors), does not require training, supports incremental insertion, and has well-understood tuning parameters. The main trade-off is memory: HNSW indexes are typically 1.5-2x the size of the raw vector data. It is the default choice for most applications and is supported by [pgvector](https://github.com/pgvector/pgvector), [FAISS](https://github.com/facebookresearch/faiss), [Qdrant](https://qdrant.tech/), [Weaviate](https://weaviate.io/), and most other vector databases.

**IVF (Inverted File Index)** partitions the vector space into clusters (using k-means or a similar algorithm) and assigns each vector to its nearest cluster centroid. At query time, only the vectors in the closest clusters are searched, reducing the search space dramatically. IVF requires a training step (to compute the centroids) and works best when the number of clusters is tuned to the data size. It is more memory-efficient than HNSW and can be combined with product quantization (IVF-PQ) for further compression, making it a good choice for very large collections (hundreds of millions to billions of vectors) where memory is a constraint. The trade-off is that recall is more sensitive to parameter tuning, and IVF does not support efficient incremental updates -- adding new vectors may require retraining the centroids.

In practice, start with HNSW unless you have a specific reason to prefer IVF (very large collections, tight memory budgets). Most vector databases use HNSW as their default index type.

## Practical Design Choices

### Chunking And Metadata

The quality of your vector search is determined as much by how you prepare your data as by the embedding model or index algorithm you choose. Chunking -- the process of splitting documents into pieces for embedding -- is where most retrieval quality is won or lost.

Chunk by semantic boundaries, not arbitrary character counts. A paragraph that discusses a single concept makes a good chunk. A chunk that starts mid-sentence in one topic and ends mid-sentence in another will produce a muddled embedding that is not close to any relevant query. Good strategies include splitting on paragraph boundaries, section headers, or sentence boundaries with a target length (200-500 tokens is a common range). Overlap between adjacent chunks (e.g., 50 tokens of overlap) helps ensure that concepts split across chunk boundaries are still findable.

Store rich metadata alongside each vector. At minimum, include the source document ID, the chunk's position within the document, a timestamp, and any access-control labels. In production, you will need this metadata for filtering (only search documents the user has permission to see), debugging (which chunk produced this answer?), and freshness (prefer recent documents over stale ones). Metadata filtering happens before or during the vector search and is essential for both correctness and security.

Enforce access control at query time. If your knowledge base contains documents with different permission levels, you must filter results based on the querying user's permissions before passing them to the LLM. This is not optional and cannot be deferred to the generation step -- once a document's content is in the model's context, it will use it regardless of who is asking.

### Retrieval Strategy

Start simple and add complexity only when measurements show it is needed.

**Baseline: vector similarity + metadata filters.** Embed the query, search the index with appropriate metadata filters (ACL, recency, department), and return the top-k results. This works well for most use cases and is easy to debug and maintain.

**Add a reranker when precision matters.** A reranker is a cross-encoder model that takes the query and a candidate document as input and produces a relevance score. Unlike embedding similarity (which compares independent representations of the query and document), a reranker considers the query and document together, which allows it to capture more nuanced relevance signals. The typical pattern is to retrieve a larger set of candidates with vector search (e.g., top-50) and then rerank them to get a smaller, higher-precision set (e.g., top-5). Reranking adds latency and cost, so use it when precision measurably improves your end-to-end quality.

**Consider hybrid search for keyword-sensitive queries.** Pure vector search can miss exact terms that matter -- product names, error codes, acronyms. Hybrid search combines vector similarity with traditional keyword search ([BM25](https://en.wikipedia.org/wiki/Okapi_BM25) or similar) by running both and merging the results. This is particularly valuable for short queries and queries containing specific terms that must appear in the result. Most vector databases now support hybrid search natively.

### Index Management

Treat your vector index as a versioned artifact, much like a database schema or a deployed model.

**Version the embedding model.** When you change embedding models, all existing vectors become incompatible. Plan for a full re-embedding and index rebuild when upgrading. Keep the model version as metadata on the index so you can detect mismatches.

**Plan for incremental updates.** Documents are added, updated, and deleted over time. HNSW indexes support incremental insertion efficiently. IVF indexes may need periodic retraining. Design your pipeline to handle updates without full rebuilds where possible, and schedule periodic full rebuilds to clean up fragmentation.

**Monitor index health.** Track recall on a held-out evaluation set, query latency at various percentiles, index size, and the rate of new inserts vs. deletes. Degradation in any of these signals indicates it is time to rebuild or retune.

### Evaluation

Retrieval quality must be measured, not assumed. There are two levels of evaluation to consider.

**Retrieval evaluation** measures how well your vector search finds the right documents. The standard metrics are recall@k (what fraction of relevant documents appear in the top k results?) and precision@k (what fraction of the top k results are relevant?). Both require a labeled evaluation set: a collection of queries paired with the documents that should be retrieved for each query. Building this set is tedious but essential -- without it, you are tuning blind.

**End-to-end evaluation** measures whether your full pipeline (retrieval + generation) produces correct, faithful, and useful answers. This includes checking that the model's answer is supported by the retrieved documents (faithfulness), that the answer actually addresses the user's question (relevance), and that the answer does not include information that was not in the retrieved context (groundedness). See the [Evals And Testing](05-evals.md) chapter for how to build these evaluation pipelines.

## Pitfalls

**Garbage in, garbage out.** Poor chunking and noisy text are the most common causes of bad retrieval quality. HTML artifacts, boilerplate headers and footers, OCR errors, and tables that do not survive text extraction all produce embeddings that pollute your vector space. Invest in preprocessing: clean your text, remove boilerplate, and verify that your chunks read as coherent passages.

**Missing metadata makes debugging and access control harder.** When a user reports a bad answer, you need to trace it back to the retrieved chunks and the source document. Without metadata, this is nearly impossible. Without ACL metadata, you risk leaking information across permission boundaries. Add metadata from the start -- retrofitting it is much harder.

**Embedding model changes silently shift neighbors.** If you update your embedding model without re-embedding your corpus, queries will be embedded in one vector space while documents are in another. The results will silently degrade. Always version your embedding model and treat model changes as index-rebuild events.

**Over-relying on vector search for exact match.** A user searching for error code "ERR-4032" expects an exact match, not a semantic approximation. Pure vector search may return documents about error handling in general rather than the specific error code. Use hybrid search or metadata filters for queries where exact terms matter.

**Ignoring the cost of re-embedding.** For large corpora (millions of documents), re-embedding is a significant cost in both time and API spend. Factor this into your embedding model selection and update frequency. A slightly less capable model that you can afford to re-embed is more valuable than a perfect model that locks you into a stale index.

## Checklist
- Do you version the embedding model and the index build?
- Do you log retrieved IDs (not raw content) for debugging?
- Do you have a labeled retrieval eval set?
- Do you have reranking for hard queries?
- Do you enforce ACL filtering during retrieval?
- Is your chunking strategy based on semantic boundaries, not arbitrary character counts?
- Do you store sufficient metadata for debugging, access control, and freshness filtering?
- Do you have a plan for index rebuilds when the embedding model changes?
- Do you monitor retrieval quality (recall@k) in production?

## References
- FAISS (vector similarity search library). https://github.com/facebookresearch/faiss
- FAISS paper (efficient similarity search and clustering). https://arxiv.org/abs/2401.08281
- HNSW paper (graph-based ANN). https://arxiv.org/abs/1603.09320
- Sentence-BERT paper (strong baseline for embeddings). https://arxiv.org/abs/1908.10084
- pgvector (Postgres vector extension). https://github.com/pgvector/pgvector

*Last audited: 2026-02-10 Â· [Audit methodology](23-audit-methodology.md)*

---
[Contents](README.md) | [Prev](11-structured-outputs-and-tool-calling.md) | [Next](17-fine-tuning.md)
